{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"startup","text":"<p>Materials to help new students get started in the stars and planets group at UQ!</p>"},{"location":"#host","title":"Host","text":"<p>Benjamin Pope</p>"},{"location":"#overview","title":"Overview","text":"<p>If you're reading this, welcome! I'm excited to have you joining our group at UQ. </p> <p>These (evolving) documents are to help you get to know the people, tools, ideas, and values that will be important in working together on these projects. These are my personal views, and open to discussion and revision. I'm quite junior myself so don't take my advice as gospel! Check out the links on the left to browse the resources.</p>"},{"location":"#the-general-idea","title":"The General Idea","text":"<p>This is, insofar as is possible, a non-hierarchical group. I'm not going to tell you what to do! My role is to help you learn as a researcher.</p>"},{"location":"#license","title":"License","text":"<p>We invite anyone interested to use and modify these startup docs under a MIT license. If you want to add anything, it would be great if you submit a pull request so we can update the page with new materials!</p>"},{"location":"jax_install/","title":"Jax install","text":"<p>The Jax team ran out of space on PyPI so they are now semi-regularly deleting old versions of jax and jaxlib from pip. To find current supported versions, use this jaxlib \u00b7 PyPI link.</p> <p>They discuss this on the github discussions page: jaxlib removal from pypi \u00b7 Discussion #7608 \u00b7 google/jax \u00b7 GitHub details the removal, and provides basic instructions about how to install depreciated versions.</p> <p>To install old versions first navigate to this website and find the relvelant version. Note the cuda versions include GPU support, while the nocuda version is the standard version. Say we want version 0.1.57, there are multiple distributions:</p> <p>nocuda/jaxlib-0.1.57-cp36-none-manylinux2010_x86_64.whl nocuda/jaxlib-0.1.57-cp37-none-manylinux2010_x86_64.whl nocuda/jaxlib-0.1.57-cp38-none-manylinux2010_x86_64.whl nocuda/jaxlib-0.1.57-cp39-none-manylinux2010_x86_64.whl</p> <p>Since we don't know which one we have to try all of them. Install to your venv like so:</p> <pre><code>pip install https://storage.googleapis.com/jax-releases/nocuda/jaxlib-0.1.57-cp38-none-manylinux2010_x86_64.whl\n</code></pre> <p>cp38 turns out the be the correct version for OSX, with the other versions throwing an error:</p> <pre><code>ERROR: jaxlib-0.1.57-cp39-none-manylinux2010_x86_64.whl is not a supported wheel on this platform\n</code></pre> <p>On the github discussion page they also show how to update requirements.txt files to search this google storage drive for the required version. For exmaple this would be how to install the correct jax versions for the present (as of 02/09/2021) stable Morphine distribution</p> <pre><code># jax\n--find-links https://storage.googleapis.com/jax-releases/jax_releases.html\njax==0.1.77\njaxlib==0.1.57\n</code></pre>"},{"location":"laplace/","title":"Laplace","text":"In\u00a0[10]: Copied! <pre>import jax.numpy as np\nfrom jax import jit, grad, vmap, hessian\n\nimport matplotlib.pyplot as plt\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import multivariate_normal\n\nfrom chainconsumer import ChainConsumer, Chain\n</pre> import jax.numpy as np from jax import jit, grad, vmap, hessian  import matplotlib.pyplot as plt  from scipy.optimize import minimize from scipy.stats import multivariate_normal  from chainconsumer import ChainConsumer, Chain In\u00a0[16]: Copied! <pre>def logistic(x):\n    # logistic function\n    return (1. + np.exp(-x)) ** -1.0\n\n# differentiate with jax!\ngrad_logistic = grad(logistic)\nvectorized_grad_logistic = vmap(grad_logistic)\n\nprint(logistic(0.0))\nprint(grad_logistic(0.0))\n</pre> def logistic(x):     # logistic function     return (1. + np.exp(-x)) ** -1.0  # differentiate with jax! grad_logistic = grad(logistic) vectorized_grad_logistic = vmap(grad_logistic)  print(logistic(0.0)) print(grad_logistic(0.0)) <pre>0.5\n0.25\n</pre> In\u00a0[18]: Copied! <pre>second_order_grad_logistic = vmap(grad(grad(logistic)))\n\nx = np.linspace(-10.0, 10.0, 1000)\nfig, ax = plt.subplots()\nax.plot(x, logistic(x), label=\"$f(x)$\")\nax.plot(x, vectorized_grad_logistic(x), label=\"$f'(x)$\")\nax.plot(x, second_order_grad_logistic(x), label=\"$f''(x)$\")\n_ = ax.legend()\n_ = ax.set_xlabel(\"$x$\")\n</pre> second_order_grad_logistic = vmap(grad(grad(logistic)))  x = np.linspace(-10.0, 10.0, 1000) fig, ax = plt.subplots() ax.plot(x, logistic(x), label=\"$f(x)$\") ax.plot(x, vectorized_grad_logistic(x), label=\"$f'(x)$\") ax.plot(x, second_order_grad_logistic(x), label=\"$f''(x)$\") _ = ax.legend() _ = ax.set_xlabel(\"$x$\")  In\u00a0[19]: Copied! <pre>from jax import jit\n\n@jit\ndef jit_logistic(x):\n    # logistic function\n    return (1. + np.exp(-x)) ** -1.0\n\n@jit\ndef jit_grad_logistic(x):\n    # compile the gradient as well\n    return grad(logistic)(x)\n</pre> from jax import jit  @jit def jit_logistic(x):     # logistic function     return (1. + np.exp(-x)) ** -1.0  @jit def jit_grad_logistic(x):     # compile the gradient as well     return grad(logistic)(x)  In\u00a0[20]: Copied! <pre>sigma = np.array([(1.0, 0.5), (0.5, 1.0)])\nmu = np.array([0.5, 2.0])\nnu = np.array([7])\n\nsigma_inv = np.linalg.inv(sigma)\n\ndef log_posterior(theta):\n    return np.log(\n            1.0 + nu ** -1.0 * np.dot((theta - mu), np.dot(sigma_inv, (theta - mu).T).T)\n        ) * (0.5  * -(nu + theta.shape[0]))\n\n# plot the distribution\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\n\nX, Y = np.meshgrid(x, y)\nXY = np.stack((X, Y)).reshape(2, 10000).T\n\nZ = vmap(log_posterior, in_axes=0)(XY).reshape(100, 100)\n\nfig, ax = plt.subplots()\nax.contourf(X, Y, Z)\nax.set_xlabel(r\"$\\theta_0$\")\n_ = ax.set_ylabel(r\"$\\theta_1$\")\n</pre> sigma = np.array([(1.0, 0.5), (0.5, 1.0)]) mu = np.array([0.5, 2.0]) nu = np.array([7])  sigma_inv = np.linalg.inv(sigma)  def log_posterior(theta):     return np.log(             1.0 + nu ** -1.0 * np.dot((theta - mu), np.dot(sigma_inv, (theta - mu).T).T)         ) * (0.5  * -(nu + theta.shape[0]))  # plot the distribution x = np.linspace(-10, 10, 100) y = np.linspace(-10, 10, 100)  X, Y = np.meshgrid(x, y) XY = np.stack((X, Y)).reshape(2, 10000).T  Z = vmap(log_posterior, in_axes=0)(XY).reshape(100, 100)  fig, ax = plt.subplots() ax.contourf(X, Y, Z) ax.set_xlabel(r\"$\\theta_0$\") _ = ax.set_ylabel(r\"$\\theta_1$\")  In\u00a0[21]: Copied! <pre>@jit\ndef negative_log_posterior(theta):\n    # negative log posterior to minimise\n    return (-np.log(\n        1.0 + nu ** -1.0 * np.dot((theta - mu), np.dot(sigma_inv, (theta - mu).T).T)\n    ) * (0.5  * -(nu + theta.shape[0])))[0]\n\n@jit\ndef grad_negative_log_posterior(theta):\n    # gradient of the negative log posterior\n    return grad(negative_log_posterior)(theta)\n\n@jit\ndef approx_covariance_matrix(theta):\n    # evaluate the covariance matrix of the approximate normal\n    return np.linalg.inv(hessian(negative_log_posterior)(theta))\n\n# go!\ntheta_star = minimize(\n    negative_log_posterior, \n    np.array([0.0, 0.0]), \n    jac=grad_negative_log_posterior, \n    method=\"BFGS\"\n).x\n\nsigma_approx = approx_covariance_matrix(theta_star)\n</pre>  @jit def negative_log_posterior(theta):     # negative log posterior to minimise     return (-np.log(         1.0 + nu ** -1.0 * np.dot((theta - mu), np.dot(sigma_inv, (theta - mu).T).T)     ) * (0.5  * -(nu + theta.shape[0])))[0]  @jit def grad_negative_log_posterior(theta):     # gradient of the negative log posterior     return grad(negative_log_posterior)(theta)  @jit def approx_covariance_matrix(theta):     # evaluate the covariance matrix of the approximate normal     return np.linalg.inv(hessian(negative_log_posterior)(theta))  # go! theta_star = minimize(     negative_log_posterior,      np.array([0.0, 0.0]),      jac=grad_negative_log_posterior,      method=\"BFGS\" ).x  sigma_approx = approx_covariance_matrix(theta_star)  In\u00a0[22]: Copied! <pre>from scipy.stats import norm\nfrom scipy.stats import t\n\nfig = plt.figure(constrained_layout=True, figsize=(15, 10))\nspec = fig.add_gridspec(ncols=2, nrows=2)\nfig.subplots_adjust(hspace=0, wspace=0)\n\nax3 = fig.add_subplot(spec[0, 0])\nax2 = fig.add_subplot(spec[1, 1])\nax1 = fig.add_subplot(spec[1, 0])\n\ncontour = ax1.contour(\n    X, Y, Z / Z.max(), colors=\"0.4\", levels=15, linestyles=\"-\", linewidths=3\n)\n\n# calculate the density of the approximating Normal distribution\nZ_0 = (\n    multivariate_normal(mean=theta_star, cov=sigma_approx).logpdf(XY).reshape(100, 100)\n)\n\nax1.contour(\n    X, Y, Z_0 / Z_0.max(), colors=\"#2c7fb8\", levels=15, linestyles=\"--\", linewidths=3\n)\n\nax1.set_xlabel(r\"$\\theta_0$\")\nax1.set_ylabel(r\"$\\theta_1$\")\n\ntheta_grid = np.linspace(-10,10,1000)\n\n\nax2.plot(\n    norm.pdf(theta_grid, theta_star[1], np.sqrt(sigma_approx[1, 1])),\n    theta_grid,\n    c=\"#2c7fb8\",\n    ls=\"--\",\n    lw=3,\n)\nax2.plot(\n    t.pdf(theta_grid, nu[1], mu[1], np.sqrt(sigma[1, 1])), theta_grid, c=\"0.4\", lw=3\n)\n\n\nax3.plot(\n    theta_grid,\n    norm.pdf(theta_grid, theta_star[0], np.sqrt(sigma_approx[0, 0])),\n    c=\"#2c7fb8\",\n    ls=\"--\",\n    lw=3,\n    label=\"Laplace\",\n)\nax3.plot(\n    theta_grid,\n    t.pdf(theta_grid, nu[0], mu[0], np.sqrt(sigma[0, 0])),\n    c=\"0.4\",\n    lw=3,\n    label=\"Exact\",\n)\nax3.legend()\n\nax3.set_xlim(-10, 10)\nax2.set_ylim(-10, 10)\n\nax2.xaxis.set_visible(False)\nax3.yaxis.set_visible(False)\n</pre> from scipy.stats import norm from scipy.stats import t  fig = plt.figure(constrained_layout=True, figsize=(15, 10)) spec = fig.add_gridspec(ncols=2, nrows=2) fig.subplots_adjust(hspace=0, wspace=0)  ax3 = fig.add_subplot(spec[0, 0]) ax2 = fig.add_subplot(spec[1, 1]) ax1 = fig.add_subplot(spec[1, 0])  contour = ax1.contour(     X, Y, Z / Z.max(), colors=\"0.4\", levels=15, linestyles=\"-\", linewidths=3 )  # calculate the density of the approximating Normal distribution Z_0 = (     multivariate_normal(mean=theta_star, cov=sigma_approx).logpdf(XY).reshape(100, 100) )  ax1.contour(     X, Y, Z_0 / Z_0.max(), colors=\"#2c7fb8\", levels=15, linestyles=\"--\", linewidths=3 )  ax1.set_xlabel(r\"$\\theta_0$\") ax1.set_ylabel(r\"$\\theta_1$\")  theta_grid = np.linspace(-10,10,1000)   ax2.plot(     norm.pdf(theta_grid, theta_star[1], np.sqrt(sigma_approx[1, 1])),     theta_grid,     c=\"#2c7fb8\",     ls=\"--\",     lw=3, ) ax2.plot(     t.pdf(theta_grid, nu[1], mu[1], np.sqrt(sigma[1, 1])), theta_grid, c=\"0.4\", lw=3 )   ax3.plot(     theta_grid,     norm.pdf(theta_grid, theta_star[0], np.sqrt(sigma_approx[0, 0])),     c=\"#2c7fb8\",     ls=\"--\",     lw=3,     label=\"Laplace\", ) ax3.plot(     theta_grid,     t.pdf(theta_grid, nu[0], mu[0], np.sqrt(sigma[0, 0])),     c=\"0.4\",     lw=3,     label=\"Exact\", ) ax3.legend()  ax3.set_xlim(-10, 10) ax2.set_ylim(-10, 10)  ax2.xaxis.set_visible(False) ax3.yaxis.set_visible(False)  <pre>/var/folders/vx/lm_q_1ld7c13_fbqfscs9n4w0000gq/T/ipykernel_34918/2885421084.py:6: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\n  fig.subplots_adjust(hspace=0, wspace=0)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[32]: Copied! <pre># use Chainconsumer to plot the covariance matrix\n\nc = ChainConsumer()\nc.add_chain(Chain.from_covariance(mean=np.zeros(sigma_approx.shape[0]),covariance=sigma_approx,columns=[r\"$\\theta_0$\", r\"$\\theta_1$\"],name=\"Laplace\"))\n\nc.add_chain(chain)\n</pre> # use Chainconsumer to plot the covariance matrix  c = ChainConsumer() c.add_chain(Chain.from_covariance(mean=np.zeros(sigma_approx.shape[0]),covariance=sigma_approx,columns=[r\"$\\theta_0$\", r\"$\\theta_1$\"],name=\"Laplace\"))  c.add_chain(chain) <pre>/Users/benpope/opt/anaconda3/envs/jax/lib/python3.11/site-packages/chainconsumer/chain.py:291: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n  samples = rng.multivariate_normal(mean, covariance, size=1000000)  # type: ignore\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"laplace/","title":"A brief introduction to Laplace's method in Jax","text":"","tags":["Statistics","Computing"]},{"location":"laplace/#introduction","title":"Introduction","text":"<p>This is an adaptation of a blog by my colleague Angus Williams, which is no longer up. In it, we describe how Jax can be used to implement an important method in Bayesian statistics: Laplace's approximation.</p>","tags":["Statistics","Computing"]},{"location":"laplace/#the-laplace-approximation","title":"The Laplace approximation","text":"<p>Imagine we have some probability model with some parameters \\(\\theta\\), and we've constrained the model using some data \\(D\\). In Bayesian inference, our goal is always to calculate integrals like this:</p> \\[\\mathbb{E}\\left[h(\\theta)\\right] = \\int \\mathrm{d}\\theta \\, h(\\theta) \\, p(\\theta | D)\\] <p>we are interested in the expectation of some function \\(h(\\theta)\\) with respect to the posterior distribution \\(p(\\theta | D)\\).  For interesting models, the posterior is complex, and so we have no hope of calculating these integrals analytically. Because of this, Bayesians have devised many methods for approximating them.  If you've got time, the best thing to do is use Markov Chain Monte Carlo.  But, if your dataset is quite large relative to your time and computational budget, you may need to try something else. A typical choice is Variational Inference. </p> <p>Another, possibly less talked-about, approach is called Laplace's approximation. It works really well when you have quite a lot of data because of the Bayesian central limit theorem. In this approach, we approximate the posterior distribution by a Normal distribution. This is a common approximation (it's often used in Variational Inference too), but Laplace's method has a specific way of finding the Normal distribution that best matches the posterior.</p> <p>Suppose we know the location \\(\\theta^*\\) of the maximum point of the posterior<sup>1</sup>. Now let's Taylor expand the log posterior around this point. To reduce clutter, I'll use the notation \\(\\log p(\\theta | D) \\equiv f(\\theta)\\). For simplicity, let's consider the case when \\(\\theta\\) is scalar:</p> \\[ f(\\theta)  \\approx  f(\\theta^*)  + \\frac{\\partial f}{\\partial \\theta}\\bigg|_{\\theta^*}\\,(\\theta - \\theta^*) + \\dfrac{1}{2}\\frac{\\partial^2 f}{\\partial \\theta^2}\\bigg|_{\\theta^*}\\,(\\theta - \\theta^*)^2 \\\\ = f(\\theta^*)  + \\dfrac{1}{2}\\frac{\\partial^2 f}{\\partial \\theta^2}\\bigg|_{\\theta^*}\\,(\\theta - \\theta^*)^2 \\] <p>The first derivative disappears because \\(\\theta^*\\) is a maximum point, so the gradient there is zero.  Let's compare this to the logarithm of a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), which I'll call \\(g(\\theta)\\):</p> \\[ g(\\theta) = -\\frac{1}{2}\\log (2\\pi\\sigma^2) - \\dfrac{1}{2}\\dfrac{1}{\\sigma^2}(\\theta - \\mu)^2 \\] <p>We can match up the terms in the expressions for \\(g(\\theta)\\) and the Taylor expansion of \\(f(\\theta)\\) (ignoring the constant additive terms) to see that</p> \\[ \\mu = \\theta^* \\\\ \\sigma^2 = \\left(-\\dfrac{\\partial^2 f}{\\partial \\theta^2}\\bigg|_{\\theta^*}\\right)^{-1} \\] <p>Consequently, we might try approximating the posterior distribution with a Normal distribution, and set the mean and variance to these values. In multiple dimensions, the covariance matrix of the resulting multivariate normal is the inverse of the Hessian matrix of the negative log posterior at \\(\\theta^*\\):</p> \\[\\Sigma_{ij} = \\dfrac{\\partial ^2 (-f)}{\\partial \\theta_i \\partial \\theta_j}^{-1}\\bigg|_{\\theta^*}\\] <p>Already, we can see that Laplace's approximation requires us to be able to twice differentiate the posterior distribution in order to obtain \\(\\sigma\\). In addition, we have to find the location \\(\\theta^* \\) of the maximum of the posterior. We probably have to do this numerically, which means using some kind of optimisation routine. The most efficient of these optimisation routines require the gradient of the objective function. So, using Laplace's approximation means we want to evaluate the first and second derivatives of the posterior. Sounds like a job for <code>jax</code>!</p>","tags":["Statistics","Computing"]},{"location":"laplace/#example-a-student-t-posterior-distribution","title":"Example: a Student-t posterior distribution","text":"<p>Suppose our true posterior is a 2D Student-t:</p> \\[ p(\\theta | D)  \\propto  \\left(1+\\frac{1}{\\nu}(\\theta - \\mu)^T \\Sigma^{-1}(\\theta - \\mu)\\right)^{-(\\nu + \\mathrm{dim}(\\theta))/2} \\] <p>This is a simple example, and we can actually sample from a Student-t rather easily. Nevertheless, let's go ahead and use it to implement Laplace's method in <code>jax</code>. Let's set the values of the constants in the Student-t:</p> \\[ \\mu = \\begin{pmatrix} 0.5 \\\\ 2 \\end{pmatrix} \\\\ \\Sigma = \\begin{pmatrix} 1 &amp; 0.5 \\\\ 0.5 &amp; 1 \\end{pmatrix} \\\\ \\nu = 7 \\] <p>To implement this in <code>jax</code>, we first import all the relevant packages.</p> Import Dependencies <pre><code>import jax.numpy as np\nfrom jax import jit, grad, vmap, hessian\n\nimport matplotlib.pyplot as plt\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import multivariate_normal\n\nfrom chainconsumer import ChainConsumer, Chain\n</code></pre> <p>First, let's plot the log posterior:</p> <pre><code># choose some values for the Student-t\nsigma = np.array([(1.0, 0.5), (0.5, 1.0)])\nmu = np.array([0.5, 2.0])\nnu = np.array([7])\n\nsigma_inv = np.linalg.inv(sigma)\n\ndef log_posterior(theta):\n    return np.log(\n            1.0 + nu ** -1.0 * np.dot((theta - mu), np.dot(sigma_inv, (theta - mu).T).T)\n        ) * (0.5  * -(nu + theta.shape[0]))\n</code></pre> Plotting Code <pre><code># plot the distribution\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\n\nX, Y = np.meshgrid(x, y)\nXY = np.stack((X, Y)).reshape(2, 10000).T\n\nZ = vmap(log_posterior, in_axes=0)(XY).reshape(100, 100)\n\nfig, ax = plt.subplots()\nax.contourf(X, Y, Z)\nax.set_xlabel(r\"$\\theta_0$\")\n_ = ax.set_ylabel(r\"$\\theta_1$\")\n</code></pre> <p></p> <p>Now let's implement Laplace's method in <code>jax</code>:</p> <pre><code>@jit\ndef negative_log_posterior(theta):\n    # negative log posterior to minimise\n    return (-np.log(\n        1.0 + nu ** -1.0 * np.dot((theta - mu), np.dot(sigma_inv, (theta - mu).T).T)\n    ) * (0.5  * -(nu + theta.shape[0])))[0]\n\n@jit\ndef grad_negative_log_posterior(theta):\n    # gradient of the negative log posterior\n    return grad(negative_log_posterior)(theta)\n\n@jit\ndef approx_covariance_matrix(theta):\n    # evaluate the covariance matrix of the approximate normal\n    return np.linalg.inv(hessian(negative_log_posterior)(theta))\n\n# go!\ntheta_star = minimize(\n    negative_log_posterior, \n    np.array([0.0, 0.0]), \n    jac=grad_negative_log_posterior, \n    method=\"BFGS\"\n).x\n\nsigma_approx = approx_covariance_matrix(theta_star)\n</code></pre> <p>This is a very short piece of code! I had to define the negative log posterior (and JIT compiled it for speed), since we will minimise this to find \\(\\theta^*\\). Then, I used <code>jax</code>'s <code>grad</code> function to differentiate this once, so that we can used a gradient-based optimiser. Next, I used <code>jax</code>'s <code>hessian</code> function to find the covariance matrix for our approximating normal.  Finally, I used scipy's <code>minimize</code> function to find the optimal point \\(\\theta^*\\). </p> <p>Note that this code is actually rather general! As long as the function <code>negative_log_posterior</code> can be implemented in a way that <code>jax</code> can differentiate (which it probably can), then the rest of the code stays exactly the same! Let's have a look at how good our normal approximation is:</p> Plotting code <pre><code>from scipy.stats import norm\nfrom scipy.stats import t\n\nfig = plt.figure(constrained_layout=True, figsize=(15, 10))\nspec = fig.add_gridspec(ncols=2, nrows=2)\nfig.subplots_adjust(hspace=0, wspace=0)\n\nax3 = fig.add_subplot(spec[0, 0])\nax2 = fig.add_subplot(spec[1, 1])\nax1 = fig.add_subplot(spec[1, 0])\n\ncontour = ax1.contour(\n    X, Y, Z / Z.max(), colors=\"0.4\", levels=15, linestyles=\"-\", linewidths=3\n)\n\n# calculate the density of the approximating Normal distribution\nZ_0 = (\n    multivariate_normal(mean=theta_star, cov=sigma_approx).logpdf(XY).reshape(100, 100)\n)\n\nax1.contour(\n    X, Y, Z_0 / Z_0.max(), colors=\"#2c7fb8\", levels=15, linestyles=\"--\", linewidths=3\n)\n\nax1.set_xlabel(r\"$\\theta_0$\")\nax1.set_ylabel(r\"$\\theta_1$\")\n\n\nax2.plot(\n    norm.pdf(theta_grid, theta_star[1], np.sqrt(sigma_approx[1, 1])),\n    theta_grid,\n    c=\"#2c7fb8\",\n    ls=\"--\",\n    lw=3,\n)\nax2.plot(\n    t.pdf(theta_grid, nu[1], mu[1], np.sqrt(sigma[1, 1])), theta_grid, c=\"0.4\", lw=3\n)\n\n\nax3.plot(\n    theta_grid,\n    norm.pdf(theta_grid, theta_star[0], np.sqrt(sigma_approx[0, 0])),\n    c=\"#2c7fb8\",\n    ls=\"--\",\n    lw=3,\n    label=\"Laplace\",\n)\nax3.plot(\n    theta_grid,\n    t.pdf(theta_grid, nu[0], mu[0], np.sqrt(sigma[0, 0])),\n    c=\"0.4\",\n    lw=3,\n    label=\"Exact\",\n)\nax3.legend()\n\nax2.xaxis.set_visible(False)\nax3.yaxis.set_visible(False)\n</code></pre> <p></p> <p>At least by eye, the approximation seems reasonable. Of course, I have rather cheated here since a Student-t approaches a normal distribution as \\(\\nu \\rightarrow \\infty\\). Nonetheless, it's still pleasing to see that the numerical implementation with <code>jax</code> and scipy works as expected.</p>","tags":["Statistics","Computing"]},{"location":"laplace/#using-chainconsumer-or-corner","title":"Using ChainConsumer or Corner","text":"<p>While above we used custom plotting code to make a corner plot overlaying the true and approximate distributions, if you're using the popular ChainConsumer library, there are convenience functions for plotting Laplace/Fisher derived distributions:</p> <p><pre><code>c = ChainConsumer()\n\nc.add_chain(Chain.from_covariance(mean=theta_star,covariance=sigma_approx,columns=[r\"$\\theta_0$\", r\"$\\theta_1$\"],name=\"Laplace\"))\n\nc.plotter.plot();\n</code></pre> </p> <p>If you're using corner.py or pairplots to my knowledge, there aren't convenience functions like this - but you can just sample from the multivariate normal and pass it:</p> <pre><code>import numpy as onp\nimport corner\n\nsamples = onp.random.multivariate_normal(theta_star, sigma_approx, 10000)\n\nfigure = corner.corner(\n    samples,\n    labels=[\n        r\"$\\theta_0$\",\n        r\"$\\theta_1$\",\n    ],\n    quantiles=[0.16, 0.5, 0.84],\n    show_titles=True,\n    title_kwargs={\"fontsize\": 12},\n)\n</code></pre> <p></p> <ol> <li> <p>I'm assuming there's only one maximum, but in reality there might be several if the posterior is multimodal. Multimodality is a pain, and Laplace's approximation won't do as well in this case (in fact most methods in Bayesian statistics share this weakness).\u00a0\u21a9</p> </li> </ol>","tags":["Statistics","Computing"]},{"location":"publishing/","title":"Astronomy Publishing","text":"<p>Unlike some sciences, astronomy has a pretty healthy publishing ecosystem, without the same prevalence of predatory journals, paywalls, and high rejection rates. Here are some of the basics on publishing in astronomy.</p>"},{"location":"publishing/#astrophysics-data-system-ads","title":"Astrophysics Data System (ADS)","text":"<p>You can find almost any paper in astronomy using the powerful search engine ADS, the Astrophysics Data System. </p> <p>There is a lot of cool functionality here! You can search by first author, other authors, year, words in the title or abstract - but also remember to create libraries (eg this one of all my papers) to help organize things. The number one use case is finding papers - and the number two is creating BibTeX citation refs to go in your papers.</p>"},{"location":"publishing/#arxiv","title":"arXiv","text":"<p>For two decades now, nearly all physics and astronomy papers (and an increasing range of other sciences) have been uploaded to arXiv, and for astronomy in particular, to the subsection of arXiv called astro-ph. They are all free to access, from anywhere you are, and typically linked to ADS pages with all the metadata you would ever want.</p> <p>There are six subsections of astro-ph for different subject areas; three are directly relevant to my research, and I usually check the new round of papers every weekday at 2pm Brisbane time for </p> <ul> <li>astro-ph.EP - Earth and Planetary Astrophysics, </li> <li>astro-ph.IM - Instrumentation and Methods for Astrophysics, and </li> <li>astro-ph.SR - Solar and Stellar Astrophysics. </li> </ul>"},{"location":"publishing/#journals","title":"Journals","text":"<p>The first point is you'll probably rarely read journals - you'll find papers through ADS or arXiv!</p> <p>Most astronomy papers are handled in one of three sets of journals, more by geographic origin of the researchers than anything else. </p> <ul> <li>The American Astronomical Society publish AAS journals such as The Astrophysical Journal; </li> <li>the British Royal Astronomical Society publish Monthly Notices or MNRAS; </li> <li>and the European Southern Observatory have Astronomy &amp; Astrophysics or A&amp;A. </li> </ul> <p>These all have subsidiaries including Letters for short high impact papers, or Supplements for big data releases, et cetera. I tend to publish in ApJ or sometimes MNRAS. All three have basically quite high acceptance rates; you're unlikey to get a decent paper rejected, but peer reviewers will rigorously ask for detail and revisions.</p> <p>Prestige journals like Nature, Nature Astronomy and Science do have higher-profile results, correspondingly high rejection rates, and sometimes the results can be too good to be true! </p> <p>There are also regional journals, often dedicated to observatories, such as Publications of the Astronomical Society of Australia or PASA, and an American journal Publications of the Astronomical Society of the Pacific or PASP with a big emphasis on methods.</p> <p>We will be doing a lot of software development, and the Journal of Open Source Software is a great place to submit short papers documenting the cool open source software tools we make.</p> <p>Hardware papers are often in SPIE, which is the exception to all the above rules and can be paywalled or hard to access, or in JATIS.</p> <p>We also do interdisciplinary work, and my favourite journal for this is Proceedings of the Royal Society A, who publish lots of really cool applied mathematics and physics.</p>"},{"location":"reading/","title":"Learning the Background","text":""},{"location":"reading/#astrobites","title":"astrobites","text":"<p>If you want to keep up with the latest research, a good place to start is astrobites, a grad-student-run collective who do amazingly accessible blog posts about recent astronomy papers. While you get up to speed on astronomy research, it is great to read astrobites to get the gist of the broad sweep of astronomy!</p> <p>They also have more general guides: to (American!) graduate school, and to astrophyics software.</p>"},{"location":"reading/#astrobetter","title":"astrobetter","text":"<p>There is also a great page of astronomy tutorials and general advice at astrobetter, covering everything from software to jobs. </p>"},{"location":"reading/#statistics","title":"Statistics","text":"<p>We will be using a lot of stats in astronomy! </p>"},{"location":"reading/#general-statistics","title":"General Statistics","text":"<p>My favourite textbooks on statistics are </p> <ul> <li>Information Theory, Inference and Learning Algorithms, by Mackay, free online;</li> <li>Data Analysis: A Bayesian Tutorial by Sivia, for which I have a hard copy I can lend you; and</li> <li>Bayesian Logical Data Analysis for the Physical Sciences by Gregory, for which I also have a hard copy.</li> </ul> <p>There is a great series of tutorial papers by David Hogg and members of the NYC stats community on stats for astronomy:</p> <ul> <li>On MCMC: Data analysis recipes: Using Markov Chain Monte Carlo, Hogg &amp; Foreman-Mackey</li> <li>On regularized fitting: Fitting very flexible models: Linear regression with large numbers of parameters, Hogg &amp; Villar</li> <li>On Bayesian inference: Data analysis recipes: Fitting a model to data, Hogg, Bovy, &amp; Lang</li> <li>On Probability theory: Data analysis recipes: Probability calculus for inference, Hogg</li> </ul> <p>and other papers, like</p> <ul> <li>A Conceptual Introduction to Hamiltonian Monte Carlo, Betancourt</li> </ul>"},{"location":"reading/#gaussian-processes","title":"Gaussian Processes","text":"<ul> <li>The great textbook by Rasmussen &amp; Williams is freely available as a PDF online.</li> <li>Great lecture by Dan Foreman-Mackey: speakerdeck.com/dfm/an-astronomers-introduction-to-gaussian-processes-v2</li> <li>Introductory paper by Roberts et al 2012: ADS link</li> </ul>"},{"location":"reading/#miscellaneous-astronomy","title":"Miscellaneous Astronomy","text":"<ul> <li>Natasha Hurley-Walker's paper on the origin and reasoning behind the Jansky unit. </li> </ul>"},{"location":"reading/#ads-libraries","title":"ADS Libraries","text":"<p>Here are some ADS libraries listing papers directly relevant to our research topics. Probably good to have a look at these!</p> <ul> <li> <p>Photometry Papers</p> </li> <li> <p>Kernel Phase Papers</p> </li> <li> <p>Radio Stars and Exoplanets</p> </li> <li> <p>Radiocarbon Papers</p> </li> </ul> <p>Also...</p> <ul> <li> <p>My DPhil thesis!</p> </li> <li> <p>If you can read French, Frantz Martinache's Habilitation thesis has great discussion of kernel phase theory.</p> </li> </ul>"},{"location":"reading/#astronomy-in-australia-and-the-world","title":"Astronomy in Australia and The World","text":"<p>Satellites might live in a vacuum - but we certainly don't. Here are some reading materials to help get to grips with the complex relationship between astronomy, politics, culture, and power.</p> <p>Chanda Prescod-Weinstein maintains an excellent Decolonizing Science Reading List, which has a particularly deep engagement with issues about the exploitation of Indigenous people and land for telescopes, such as at the summit of Maunakea in Hawai'i. Prescod-Weinstein's book The Disordered Cosmos is available in the UQ library.</p> <p>In the Australian context, it is vital to engage with Aboriginal and Torres Strait Islander science. I strongly recommend the Warwick Thornton documentary We Don't Need A Map, the Ray Norris book Emu Dreaming, and the online resources at aboriginalastronomy.com.au.</p> <p>For the history of our our astronomical community, I have a collection of hard copy biographies and memoirs of prominent Australian and international astronomers, which you are welcome to borrow: for instance Australian radio astronomy pioneers Ruby Payne-Scott, John Bolton, Jim Peebles, and Carl Sagan. </p> <p>Astronomy is a very carbon-intensive science and some great work has been done on our climate impacts (which we need to do better at minimizing!). There is an excellent Climate Issue in Nature Astronomy, including especially this article about Australian astronomy's climate impacts (substantially from fossil electricity to run observatories and supercomputing!). </p>"},{"location":"software/","title":"Software Practices","text":"<p>Most of what we do in this research group is about, or enabled by, open-source software - so it is really good to get on top of practices and workflow for doing this.</p> <p>Most software development will be done in Python, and I recommend using Anaconda to install Python 3 and pip to manage packages. </p>"},{"location":"software/#my-github","title":"My GitHub","text":"<p>All my code is version-controlled github.com/benjaminpope - you should set up yours! </p>"},{"location":"software/#making-open-source-software","title":"Making Open-Source Software","text":"<p>Christina Hedges (Ames) has a fantastic introduction to open-source software practices for astronomy, which I won't try to compete with:</p> <ul> <li>christinahedges.github.io/astronomy_workflow</li> </ul>"},{"location":"software/#autodiff","title":"Autodiff","text":"<p>We will rely extensively on automatic differentiation, or autodiff. Like magic, we can propagate derivatives through almost arbitrary code via the chain rule - which means we can do fast, gradient-based inference, optimization, and perturbation theory.</p> <p>Here are some great tutorials:</p> <ul> <li>The Google Jax Autodiff Cookbook</li> <li>Get Started with Jax</li> <li>In the context of Dan Foreman-Mackey's code exoplanet: link</li> <li>This great explainer of autodiff by Robert Lange: link</li> <li>Awesome explainer by Max Slater: link</li> </ul>"},{"location":"tools/","title":"Astronomy Tools","text":"<p>Here are some essential web tools for astronomy.</p>"},{"location":"tools/#astrophysics-data-system-ads","title":"Astrophysics Data System (ADS)","text":"<p>You can find almost any publication in astronomy using the powerful search engine for papers ADS, the Astrophysics Data System. </p> <p>There is a lot of cool functionality here! You can search by first author, other authors, year, words in the title or abstract - but also remember to create libraries (eg this one of all my papers) to help organize things. The number one use case is finding papers - and the number two is creating BibTeX citation refs to go in your papers.</p>"},{"location":"tools/#simbad","title":"SIMBAD","text":"<p>The search engine for stars, SIMBAD, is your friend. Nearly all stars have many identifiers in many different catalogs - how do you know which is which? Search it in SIMBAD, by name, coordinates, or whatever, and you can bring up lots of metadata (including publications that refer to that star).</p>"},{"location":"tools/#vizier","title":"VizieR","text":"<p>From the same team that brought you SIMBAD comes VizieR, a tool for manipulating astronomical catalogues and databases. Want to cross-match two surveys? VizieR provides painless ways to do this! </p>"},{"location":"tools/#mast","title":"MAST","text":"<p>The Mikulski Archive for Space Telescopes archives all NASA space telescope data. You can search all NASA data on MAST using this web interface.</p>"},{"location":"tools/#web-tess-viewing-tool","title":"Web Tess Viewing Tool","text":"<p>The Web Tess Viewing Tool a great way to find out if any particular star is in TESS - and if so, when?  </p>"},{"location":"values/","title":"values","text":"<p>We are committed to an equitable, inclusive, and supportive research group. </p> <p>While we collaboratively develop this page, for the moment the Flatiron Institute astro data group values statement is a good piece to think about: values.astrodata.nyc.</p> <p>While we work out a detailed collaboration policy, I like Ruth Angus' and we will basically follow that. The main gist is - my expectation regarding authorship is that the major contributor to a paper should be first author, especially if they are a student. And regarding co-authorship, I would always prefer to be generous and include people.</p> <p>I wanted to add that I hope to foster a relatively non-hierarchical environment. It is my job to help students and collaborators achieve their goals and grow as researchers, and not to exploit people for their labour! As a result, please do not hesitate to make suggestions or requests regarding pay, funding, proposals, or to raise any problems that might arise.</p> <p>All research outputs, unless there is a really good reason otherwise, should be made publicly available with public data, open-source software, and publication on arXiv.</p>"},{"location":"writing/","title":"Writing in Astronomy","text":"<p>Writing in astronomy will have some different workflows to other fields - in particular, almost every paper is written in the formatting language LaTeX, rather than (say) Word, and we have to use specialist tools to work with this. It has a bit of a steep learning curve but because it makes it so easy to format citations and equations, it's actually a really helpful part of the workflow.</p>"},{"location":"writing/#writing-papers","title":"Writing Papers","text":"<p>Knapen et al have an excellent paper on general advice for publishing in astronomy. </p>"},{"location":"writing/#latex-guides","title":"LaTeX Guides","text":"<p>There is a good LaTeX guide on astrobites. </p>"},{"location":"writing/#overleaf","title":"Overleaf","text":"<p>People have strong views about LaTeX environments, but my preferred go-to for ease of use is Overleaf, which like Google Docs allows people to collaboratively work on the same document. </p>"},{"location":"writing/#writing-for-the-public","title":"Writing for the Public","text":"<p>It is really important not just to learn to write papers - a lot of an astronomer's job is actually writing proposals, reviews, lectures, and public engagement material. It is a great idea to practise this! One good place to start is to pitch an idea to astrobites as a guest post. For a more substantial and public facing piece, you might like to pitch to The Conversation.</p>"}]}